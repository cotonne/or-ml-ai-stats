{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c2332c1",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "## Pr√©sentation\n",
    "\n",
    " * **Objectif**: Trouver les meilleurs param√®tres $\\theta$ de la politique $\\pi_\\theta(s, a)$ qui maximise la mesure de qualit√© de la politique $J(\\theta)$.\n",
    " * Exemple de mesure de qualit√© pour √©valuer la politique (_average reward per time-step_)\n",
    " \n",
    "$\n",
    "J_{avR}(\\theta) = \\sum_s \\underbrace{d^{\\pi_\\theta}(s)}_\\textrm{Probabilit√© d'√™tre dans l'√©tat s} \\sum_a \\underbrace{\\pi_\\theta(s, a)}_\\textrm{Probabilit√© de choisir l'action a sachant qu'on est dans l'√©tat s} R^a_s\n",
    "$\n",
    "\n",
    " * Am√©lioration de $J(\\theta)$: Hill Climbing, Gradient Ascent\n",
    " * Si $\\pi_\\theta(s, a)$ est diff√©rentiable, calcul du gradient (log derivative trick) $\\nabla_\\theta \\pi_\\theta(s, a) = \\pi_\\theta(s, a) \\nabla_\\theta \\log \\pi_\\theta(s, a)$\n",
    "\n",
    "## Policy Gradient Theorem\n",
    "\n",
    " - Etant donn√©e une politique $\\pi_\\theta(s, a)$ d√©rivable\n",
    " - Etant donn√©e une fonction objective pour la qualit√© J\n",
    " - Alors, le gradient de la politique est:\n",
    " \n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}[Q^{\\pi_\\theta}(s, a) \\nabla_\\theta \\log \\pi_\\theta(s, a)]\n",
    "$\n",
    "\n",
    "## Monte-Carlo Policy Gradient (REINFORCE)\n",
    "\n",
    " - Mettre √† jour $\\theta$ via **Stochastic Gradient Ascent**\n",
    " - Utilise le **Policy Gradient Theorem**\n",
    " - Utilise le retour v_t comme une mesure non biais√© de $Q^{\\pi_\\theta}(s, a)$\n",
    " \n",
    "\n",
    "    def REINFORCE():\n",
    "        Initialise Œ∏\n",
    "        Pour chaque √©pisode {(s_1, a_1, r_2), ..., (s_t-1, a_t-1, r_t)}\n",
    "            Pour i = 1 √† t-1\n",
    "                Œ∏ <- Œ∏ + Œ± ‚àálog œÄ(s_i , a_i)v_i\n",
    "\n",
    "## Example with Logistic Policy / Two actions space\n",
    "\n",
    "R√©f√©rences: \n",
    " - https://www.oreilly.com/library/view/reinforcement-learning/9781492072386/app01.html\n",
    "  - https://www.janisklaise.com/post/rl-policy-gradients/\n",
    "\n",
    "Soit $\\phi(s,a)$, un vecteur repr√©sentant les variables issues de l'environnement.\n",
    "\n",
    "Logistic Policy:\n",
    "\n",
    "$\n",
    "\\pi_\\theta(a/s) = \\pi_\\theta(\\theta^T s) = \\frac {e^{\\theta^T s}} {1 + e^{\\theta^T s}}\n",
    "$\n",
    "\n",
    "Deriv√©e de la fonction logisitque\n",
    "\n",
    "$\n",
    "\\nabla_\\theta log(\\pi_\\theta(0 / s)) = s - s \\pi_\\theta(0 / s) \\\\\n",
    "\\nabla_\\theta log(\\pi_\\theta(1 / s)) = - s \\pi_\\theta(0 / s)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aca61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import copy\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ùúô(s, a):\n",
    "    return np.concatenate((s, np.array([a])))\n",
    "\n",
    "class SoftMaxReinforcement():\n",
    "    def __init__(self, Œ±, ùõæ):\n",
    "        self.Œ∏ = np.random.random((env.observation_space.shape[0]))\n",
    "        self.Œ± = Œ±\n",
    "        self.ùõæ = ùõæ\n",
    "    \n",
    "    def update(self, episode):\n",
    "        number_of_steps = len(episode)\n",
    "        v_i = 0\n",
    "        Œ∏ = self.Œ∏\n",
    "        G = [0] * (number_of_steps + 1)\n",
    "        for i in reversed(range(number_of_steps)):\n",
    "            G[i] = episode[i][2] + self.ùõæ * G[i+1]\n",
    "        for i in range(number_of_steps):\n",
    "            state_i, action_i, reward_i = episode[i]\n",
    "            proba_i = self.ùúã(state_i)\n",
    "            gradient = state_i - state_i * proba_i if action_i == 0 else - state_i * proba_i\n",
    "            Œ∏ = Œ∏ + self.Œ± * gradient * G[i]\n",
    "        self.Œ∏ = Œ∏\n",
    "        return gradient\n",
    "    \n",
    "    def ùúã(self, s):\n",
    "        return 1/(1 + np.exp(-s @ self.Œ∏))\n",
    "\n",
    "    \n",
    "def run(env, agent):\n",
    "    episode = []\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        ùúã_0 = agent.ùúã(state)\n",
    "        probs = [ùúã_0, 1 - ùúã_0]\n",
    "        selection_action = np.random.choice([0, 1], p=probs)\n",
    "        next_state, reward, terminated, truncated, info = env.step(selection_action)\n",
    "        episode.append((state, selection_action, reward))\n",
    "        state = next_state\n",
    "        done = terminated\n",
    "    \n",
    "    return episode\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "actions = env.action_space\n",
    "\n",
    "episodes_length = []\n",
    "gradients = []\n",
    "nb_episodes = 400\n",
    "agent = SoftMaxReinforcement(0.01, 0.99)\n",
    "\n",
    "for i in tqdm(range(nb_episodes)):\n",
    "    episode = run(env, agent)\n",
    "    gradient = agent.update(episode)\n",
    "    \n",
    "    episodes_length.append(len(episode))\n",
    "    gradients.append(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab1424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def moving_average(x, w = 1000):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "plt.plot(moving_average(episodes_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.utils.save_video import save_video\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
    "episode = run(env, agent)\n",
    "save_video(\n",
    "         env.render(),\n",
    "         \"videos\",\n",
    "         fps=env.metadata[\"render_fps\"],\n",
    "      )\n",
    "len(list(map(lambda e: e[2], episode)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
