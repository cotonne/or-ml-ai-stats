{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a606eb",
   "metadata": {},
   "source": [
    "R√©seau de neurones √† une couche\n",
    "\n",
    "Soit le jeu de donn√©es (X, y).\n",
    "\n",
    "Le r√©seau de neurones a une couche est d√©fini par: $\\hat{y} = f(W X + B)$\n",
    "\n",
    "Avec:\n",
    "\n",
    " - W: poids des neurones\n",
    " - B: biais\n",
    " - f: fonction d'activation non lin√©aire\n",
    " \n",
    "\n",
    "Dans le cas d'une r√©gression, \n",
    " \n",
    "On cherche √† minimiser l'erreur E:\n",
    "\n",
    " - Mean Square Error (r√©gression): $E = \\frac{1}{2}(y - \\hat{y})^2$\n",
    " - Cross-Entropy: $L_W(\\hat(y), y) = -\\sum y_c log(\\hat{y}_c) = -log(\\hat{y}_{c^*}) $\n",
    " - Divergence de Kullback-Leibler\n",
    "\n",
    "On va minimiser par **descente de gradient**:\n",
    "\n",
    "$W^h = W^{h-1} - \\epsilon \\frac {\\partial E} {\\partial W^{h-1}}$\n",
    "\n",
    "On calcule les d√©riv√©es partielles (chain rule):\n",
    "\n",
    "$\\frac {\\partial E} {\\partial W^{h-1}} = \\frac {\\partial E} {\\partial W^{h-1}} \\frac {\\partial E} {\\partial W^{h-1}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cbea2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris.data\r\n"
     ]
    }
   ],
   "source": [
    "!ls iris.data || wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc20c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f953ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"iris.data\", header=None, names=[\"sepal length\", \"sepal width\", \"petal length\", \"petal width\", \"class\"], dtype={\"class\": 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe479560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforme une sortie en une distribution de probabilit√©\n",
    "def softmax(s):\n",
    "    return np.exp(s)/np.sum(np.exp(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0fa6699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def ùúé(x):\n",
    "    return 1/(1+np.exp((-x).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fc944f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = data.shape[1] - 1\n",
    "categories = data[\"class\"].cat.categories.to_numpy()\n",
    "num_class = len(categories)\n",
    "W = np.random.random((num_class, num_features))\n",
    "# Biais\n",
    "W_b = np.random.random((num_class,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226ccd0",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ada69037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33718283, 0.32209291, 0.34072426])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data.iloc[0][:4].to_numpy()\n",
    "h = W @ x + W_b\n",
    "# Fonction d'activation\n",
    "s = ùúé(h)\n",
    "y = softmax(s)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2c67ad1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6591634651413831"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "≈∑ = categories == data.iloc[0][\"class\"]\n",
    "error = np.sum((y - ≈∑)**2)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1b859d",
   "metadata": {},
   "source": [
    "## Backward / R√©tropropagation de l'erreur\n",
    "\n",
    "On cherche √† minimiser l'erreur. Pouce faire, on va utiliser une descente de gradient.\n",
    "\n",
    "L'erreur est √©gale √†:\n",
    "$\n",
    "\\epsilon = \\frac {1} {2} ||≈∑ - model(x)||^2\n",
    "$\n",
    "\n",
    "Et sa d√©riv√©e:\n",
    "$\n",
    "\\delta_\\epsilon = \\frac {d¬†\\epsilon} {d x} = \\frac {\\frac {1} {2} ||≈∑ - model(x)||^2} {d x}\n",
    "$\n",
    "\n",
    "Pour calculer ce r√©sultat, nous allons utiliser la **d√©rivation de fonctions compos√©es** *(f ‚ó¶ g ‚ó¶ h)(z)* ou \"chain rule\" en anglais:\n",
    "\n",
    "$\n",
    "\\delta_\\epsilon = \\frac {\\frac {1} {2} ||≈∑ - y||^2} {d y} * \\frac {d softmax(s)} {d s} * \\frac {d \\sigma(h)} {d h} * \\frac {W * x + W_b} {x}\n",
    "$\n",
    "\n",
    "La [d√©riv√©e de softmax][1] est un peu compliqu√©e √† calculer. On mu\n",
    "\n",
    "Avec:\n",
    " * $ \\frac {\\frac {1} {2} ||≈∑ - y||^2} {d y} = ||≈∑ - y||$\n",
    " * $ \\frac {d \\sigma(h)} {d h} = \\sigma(h)(1‚àí\\sigma(h))$\n",
    " * $ \\frac {W * x + W_b} {x} = W$\n",
    " \n",
    "R√©f√©rence:\n",
    " * [1](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1)\n",
    " * [2](https://deepnotes.io/softmax-crossentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b901bc1",
   "metadata": {},
   "source": [
    "Cross Entropy Loss\n",
    "Cross entropy indicates the distance between what the model believes the output distribution should be, and what the original distribution really is. It is defined as, H(y,p)=‚àí‚àëiyilog(pi) Cross entropy measure is a widely used alternative of squared error. It is used when node activations can be understood as representing the probability that each hypothesis might be true, i.e. when the output is a probability distribution. Thus it is used as a loss function in neural networks which have softmax activations in the output layer.\n",
    "https://deepnotes.io/softmax-crossentropy\n",
    "https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59c71769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype   \n",
      "---  ------        --------------  -----   \n",
      " 0   sepal length  150 non-null    float64 \n",
      " 1   sepal width   150 non-null    float64 \n",
      " 2   petal length  150 non-null    float64 \n",
      " 3   petal width   150 non-null    float64 \n",
      " 4   class         150 non-null    category\n",
      "dtypes: category(1), float64(4)\n",
      "memory usage: 5.0 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f298e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
