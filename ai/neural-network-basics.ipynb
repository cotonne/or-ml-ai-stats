{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a606eb",
   "metadata": {},
   "source": [
    "R√©seau de neurones √† une couche\n",
    "\n",
    "Soit le jeu de donn√©es (X, y).\n",
    "\n",
    "Le r√©seau de neurones a une couche est d√©fini par: $\\hat{y} = f(W X + B)$\n",
    "\n",
    "Avec:\n",
    "\n",
    " - W: poids des neurones\n",
    " - B: biais\n",
    " - f: fonction d'activation non lin√©aire\n",
    " \n",
    "\n",
    "Dans le cas d'une r√©gression, \n",
    " \n",
    "On cherche √† minimiser l'erreur E:\n",
    "\n",
    " - Mean Square Error (r√©gression): $E = \\frac{1}{2}(y - \\hat{y})^2$\n",
    " - Cross-Entropy: $L_W(\\hat(y), y) = -\\sum y_c log(\\hat{y}_c) = -log(\\hat{y}_{c^*}) $\n",
    " - Divergence de Kullback-Leibler\n",
    "\n",
    "On va minimiser par **descente de gradient**:\n",
    "\n",
    "$W^h = W^{h-1} - \\epsilon \\frac {\\partial E} {\\partial W^{h-1}}$\n",
    "\n",
    "On calcule les d√©riv√©es partielles (chain rule):\n",
    "\n",
    "$\\frac {\\partial E} {\\partial W^{h-1}} = \\frac {\\partial E} {\\partial W^{h-1}} \\frac {\\partial E} {\\partial W^{h-1}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbea2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls iris.data || wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
    "! curl https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data -o iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"iris.data\", header=None, names=[\"sepal length\", \"sepal width\", \"petal length\", \"petal width\", \"class\"], dtype={\"class\": 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe479560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforme une sortie en une distribution de probabilit√©\n",
    "def softmax(s):\n",
    "    return np.exp(s)/np.sum(np.exp(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa6699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def ùúé(x):\n",
    "    return 1/(1+np.exp((-x).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc944f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = data.shape[1] - 1\n",
    "categories = data[\"class\"].cat.categories.to_numpy()\n",
    "num_class = len(categories)\n",
    "W = np.random.random((num_class, num_features))\n",
    "# Biais\n",
    "W_b = np.random.random((num_class,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226ccd0",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada69037",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[0][:4].to_numpy().astype('float64')\n",
    "h = W @ x + W_b\n",
    "# Fonction d'activation\n",
    "s = ùúé(h)\n",
    "≈∑ = softmax(s)\n",
    "≈∑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = categories == data.iloc[0][\"class\"]\n",
    "i = np.argmax(y)\n",
    "error = np.sum((y - ≈∑)**2)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1b859d",
   "metadata": {},
   "source": [
    "## Backward / R√©tropropagation de l'erreur\n",
    "\n",
    "On cherche √† minimiser l'erreur. Pouce faire, on va utiliser une descente de gradient.\n",
    "\n",
    "L'erreur est √©gale √†:\n",
    "$\n",
    "\\epsilon = \\frac {1} {2} ||≈∑ - model(x)||^2\n",
    "$\n",
    "\n",
    "Et sa d√©riv√©e:\n",
    "$\n",
    "\\delta_\\epsilon = \\frac {d¬†\\epsilon} {d W} = \\frac {\\frac {1} {2} ||≈∑ - model(x)||^2} {d W}\n",
    "$\n",
    "\n",
    "Pour calculer ce r√©sultat, nous allons utiliser la **d√©rivation de fonctions compos√©es** *(f ‚ó¶ g ‚ó¶ h)(z)* ou \"chain rule\" en anglais:\n",
    "\n",
    "$\n",
    "\\delta_\\epsilon = \\frac {\\frac {1} {2} ||≈∑ - y||^2} {d y} * \\frac {d softmax(s)} {d s} * \\frac {d \\sigma(h)} {d h} * \\frac {W * x + W_b} {W}\n",
    "$\n",
    "\n",
    "Avec:\n",
    " * $ \\frac {\\frac {1} {2} ||≈∑ - y||^2} {d y} = ||≈∑ - y||$\n",
    " * $ \\frac {d \\sigma(h)} {d h} = \\sigma(h)(1‚àí\\sigma(h))$\n",
    " * $ \\frac {d W * x + W_b} {d W} = x$\n",
    " \n",
    "La [d√©riv√©e de softmax][1] est un peu compliqu√©e √† calculer. On peut simplifier le calcul en utilisant comme fonction d'erreur l'entropie crois√©e $\\mathcal{L}(y, p) = - \\sum_i y_i log(p_i)$. Comme $y_i$ vaut 1 seulement pour la classe consid√©r√©e, on obtient pour la [d√©riv√©e de softmax avec l'entropie crois√©e](https://deepnotes.io/softmax-crossentropy):\n",
    "\n",
    "$ \\frac {\\mathcal{L}(y, ≈∑)} {d y} * \\frac {d softmax(s)} {d s} = \\hat{y}_i - y_i$\n",
    "\n",
    "\n",
    "R√©f√©rence:\n",
    " * [1](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1)\n",
    " * [2](https://deepnotes.io/softmax-crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c71769",
   "metadata": {},
   "outputs": [],
   "source": [
    "ùõø = np.transpose(x.reshape((1,-1))) @ ((≈∑[i] - 1) * (ùúé(h) * (1 - ùúé(h)))).reshape((1,-1))\n",
    "ùõø"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1285c",
   "metadata": {},
   "source": [
    "Pour effectuer une descente de gradient, nous allons soustraire le gradient calcul√© au param√®tre du mod√®le selon un hyperparam√®tre $\\alpha$ appell√© le taux d'apprentissage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21f0350",
   "metadata": {},
   "outputs": [],
   "source": [
    "ùõº = 0.05\n",
    "W -= ùõº * ùõø.T\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b644634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = W @ x + W_b\n",
    "# Fonction d'activation\n",
    "s = ùúé(h)\n",
    "≈∑ = softmax(s)\n",
    "error = np.sum((y - ≈∑)**2)\n",
    "error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
